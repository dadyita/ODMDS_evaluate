{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PACKAGE import multi_rouge\n",
    "from rouge_score.scoring import AggregateScore, Score\n",
    "from PACKAGE import metric_realization\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load api_key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('../../keys.json', 'r') as f:\n",
    "    api_keys = json.load(f)\n",
    "api_key = api_keys[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_tokens(articles: list):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k-0613\", openai_api_key=api_key, temperature=0.7, max_tokens=600)\n",
    "    return [llm.get_num_tokens(str(article)) for article in articles]\n",
    "\n",
    "\n",
    "path = 'QMSum/oracle'\n",
    "model_name = 'bart'\n",
    "\n",
    "# Load pred\n",
    "pred_file = model_name + '_summary.json'\n",
    "file_path = os.path.join(path, 'summary/' + pred_file)\n",
    "with open(file_path, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# Load ref\n",
    "ref_file = 'test.json'\n",
    "file_path = os.path.join(path, ref_file)\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        references = json.load(f)\n",
    "    references = [\n",
    "        data_item['Summary']\n",
    "        for data_item in references]\n",
    "print(calculate_tokens(predictions))\n",
    "print(calculate_tokens(references))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get prompt\n",
    "metric_list = ['coh', 'con', 'flu', 'rel']\n",
    "metric_type = metric_list[1]\n",
    "prompt = open('GPTeval/prompts/' + metric_type + '_detailed.txt').read()\n",
    "# Get messages\n",
    "messages = []\n",
    "for index, prediction in enumerate(predictions):\n",
    "    reference = references[index]\n",
    "    cur_prompt = prompt.replace('{{Document}}', reference).replace('{{Summary}}', prediction)\n",
    "    messages.append([{\"role\": \"system\", \"content\": cur_prompt}])\n",
    "print(len(predictions))\n",
    "print(np.mean(calculate_tokens(messages)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Print rouge squality score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rouge score\n",
    "for root, dirs, files in os.walk('SQuALITY'):\n",
    "    for file in files:\n",
    "        if file == 'bart_evaluate_squality_rouge.json':\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                rouge = f.read()\n",
    "                obj_rouge = eval(rouge)\n",
    "                print(root)\n",
    "                print(f\"rouge1:\\n{obj_rouge['rouge1'].mid.fmeasure * 100:.2f}\")\n",
    "                print(f\"rouge2:\\n{obj_rouge['rouge2'].mid.fmeasure * 100:.2f}\")\n",
    "                print(f\"rougeL:\\n{obj_rouge['rougeL'].mid.fmeasure * 100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Print rouge score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QMSum\\dense\\MIN\\evaluation\n",
      "rouge1\n",
      "0.32345118469097134\n",
      "QMSum\\dense\\MIN\\evaluation\n",
      "rouge2\n",
      "0.08996982718719525\n",
      "QMSum\\dense\\MIN\\evaluation\n",
      "rougeL\n",
      "0.17534373968372713\n",
      "QMSum\\dense\\MIN\\evaluation\n",
      "rougeLsum\n",
      "0.17587356617242386\n",
      "QMSum\\LLM-embedding\\MIN\\evaluation\n",
      "rouge1\n",
      "0.28786168325672334\n",
      "QMSum\\LLM-embedding\\MIN\\evaluation\n",
      "rouge2\n",
      "0.06530739852185932\n",
      "QMSum\\LLM-embedding\\MIN\\evaluation\n",
      "rougeL\n",
      "0.15674912768659122\n",
      "QMSum\\LLM-embedding\\MIN\\evaluation\n",
      "rougeLsum\n",
      "0.15685553676713304\n",
      "QMSum\\oracle\\evaluation\n",
      "rouge1\n",
      "0.29086849594636666\n",
      "QMSum\\oracle\\evaluation\n",
      "rouge2\n",
      "0.06927507342764289\n",
      "QMSum\\oracle\\evaluation\n",
      "rougeL\n",
      "0.15942051204427643\n",
      "QMSum\\oracle\\evaluation\n",
      "rougeLsum\n",
      "0.15952767316371966\n",
      "QMSum\\sparse\\MIN\\evaluation\n",
      "rouge1\n",
      "0.27348283753624225\n",
      "QMSum\\sparse\\MIN\\evaluation\n",
      "rouge2\n",
      "0.06166248008805346\n",
      "QMSum\\sparse\\MIN\\evaluation\n",
      "rougeL\n",
      "0.1609449384409522\n",
      "QMSum\\sparse\\MIN\\evaluation\n",
      "rougeLsum\n",
      "0.16081344797806874\n"
     ]
    }
   ],
   "source": [
    "# Print rouge score\n",
    "for root, dirs, files in os.walk('QMSum'):\n",
    "    for file in files:\n",
    "        if file == 'pri_rouge.json':\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                rouge = json.load(f)\n",
    "            for key, value in rouge.items():\n",
    "                print(root)\n",
    "                print(key)\n",
    "                print(value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Print bert score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print bert score\n",
    "for root, dirs, files in os.walk(\"QMSum\"):\n",
    "    for file in files:\n",
    "        if file == \"bart_bert_score.json\":\n",
    "            with open(os.path.join(root, file), \"r\") as f:\n",
    "                bert = json.load(f)\n",
    "            average_f1 = bert['average_f1']\n",
    "            print(root)\n",
    "            print(f\"{average_f1 * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rename"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rename\n",
    "for root, dirs, files in os.walk(\"../ODMDS_share_evaluate/SQuALITY\"):\n",
    "    if root.endswith('summary'):\n",
    "        prefix = 'newSummary_'  # 要替换的前缀\n",
    "        new_prefix = 'gpt3_summary_'  # 新的前缀\n",
    "        for filename in files:\n",
    "            if filename.startswith('newSummary_'):\n",
    "                new_filename = new_prefix + filename[len(prefix):]\n",
    "                os.rename(os.path.join(root, filename), os.path.join(root, new_filename))\n",
    "    if root.endswith('evaluation'):\n",
    "        new_prefix = 'gpt3_'  # 新的前缀\n",
    "        for filename in files:\n",
    "            if filename.startswith('evaluate'):\n",
    "                new_filename = new_prefix + filename\n",
    "                os.rename(os.path.join(root, filename), os.path.join(root, new_filename))\n",
    "    for filename in files:\n",
    "        if filename == 'all_results.json':\n",
    "            os.remove(os.path.join(root, filename))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Delete file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rename\n",
    "for root, dirs, files in os.walk(\"../ODMDS_share_evaluate\"):\n",
    "    for filename in files:\n",
    "        if 'evaluate_' in filename:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            os.remove(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Print GPT eval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric_list = ['coh', 'con', 'flu', 'rel']\n",
    "model_list = ['bart_', 'gpt3_']\n",
    "metric_index = 1\n",
    "model_index = 0\n",
    "suffix = model_list[model_index] + metric_list[metric_index] + '_gpteval.json'\n",
    "for root, dirs, files in os.walk(\"QMSum\"):\n",
    "    for file in files:\n",
    "        if file == suffix:\n",
    "            with open(os.path.join(root, file), \"r\") as f:\n",
    "                gpteval = json.load(f)\n",
    "            value_li = []\n",
    "            for key, value in gpteval.items():\n",
    "                value_li.extend(value)\n",
    "            # average = sum(value_li) / len(value_li)\n",
    "            average = gpteval['average'][0]\n",
    "            # gpteval['average'] = [average]\n",
    "            print(os.path.join(root, file))\n",
    "            print(f'{average * 10:.2f}')\n",
    "            # with open(os.path.join(root, file), \"w\") as f:\n",
    "            #     temp = json.dumps(gpteval)\n",
    "            #     f.write(temp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"QMSum\"):\n",
    "    if not files and not dirs:\n",
    "        with open(os.path.join(root, 'empty.txt'), 'w') as f:\n",
    "            f.write('1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test random index of primera"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7\n",
      "1.6\n",
      "1.7\n",
      "1.8\n"
     ]
    }
   ],
   "source": [
    "with open('QMSum/randomIndex/index.json', 'r') as f:\n",
    "    random_index_list = json.load(f)\n",
    "for root,dirs,files in os.walk('QMSum'):\n",
    "    for filename in files:\n",
    "        filepath=os.path.join(root,filename)\n",
    "        if 'pri_rel' in filename:\n",
    "            with open(filepath,'r') as f:\n",
    "                rel=json.load(f)\n",
    "            rel=[rel['Summary'][index] for index in random_index_list]\n",
    "            print(np.mean(rel))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
