{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PACKAGE import multi_rouge\n",
    "from rouge_score.scoring import AggregateScore, Score\n",
    "from PACKAGE import metric_realization\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../../keys.json', 'r') as f:\n",
    "    api_keys = json.load(f)\n",
    "api_key = api_keys[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calculate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8822.588461538462\n"
     ]
    }
   ],
   "source": [
    "def calculate_tokens(articles: list):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k-0613\", openai_api_key=api_key, temperature=0.7, max_tokens=600)\n",
    "    return [llm.get_num_tokens(str(article)) for article in articles]\n",
    "\n",
    "\n",
    "path = 'QMSum/oracle'\n",
    "model_name = 'bart'\n",
    "\n",
    "# Load pred\n",
    "pred_file = model_name + '_summary.json'\n",
    "file_path = os.path.join(path, 'summary/' + pred_file)\n",
    "with open(file_path, 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# Load ref\n",
    "ref_file = 'test.json'\n",
    "file_path = os.path.join(path, ref_file)\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        references = json.load(f)\n",
    "    references = [\n",
    "        data_item['Summary']\n",
    "        for data_item in references]\n",
    "\n",
    "with open('SQuALITY/LLM-embedding/max/test.json','r') as f:\n",
    "    data=json.load(f)\n",
    "    data=[item['Query']+item['Article']for item in data]\n",
    "print(np.mean(calculate_tokens(data)))\n",
    "# print(calculate_tokens(predictions))\n",
    "# print(calculate_tokens(references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get prompt\n",
    "metric_list = ['coh', 'con', 'flu', 'rel']\n",
    "metric_type = metric_list[1]\n",
    "prompt = open('GPTeval/prompts/' + metric_type + '_detailed.txt').read()\n",
    "# Get messages\n",
    "messages = []\n",
    "for index, prediction in enumerate(predictions):\n",
    "    reference = references[index]\n",
    "    cur_prompt = prompt.replace('{{Document}}', reference).replace('{{Summary}}', prediction)\n",
    "    messages.append([{\"role\": \"system\", \"content\": cur_prompt}])\n",
    "print(len(predictions))\n",
    "print(np.mean(calculate_tokens(messages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Print rouge squality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense\\MIN\n",
      "rouge1:31.19\n",
      "rouge2:7.54\n",
      "rougeL:16.77\n",
      "\n",
      "LLM-embedding\\MIN\n",
      "rouge1:33.32\n",
      "rouge2:8.45\n",
      "rougeL:18.06\n",
      "\n",
      "oracle\\evaluation\n",
      "rouge1:36.74\n",
      "rouge2:10.09\n",
      "rougeL:19.20\n",
      "\n",
      "sparse\\MIN\n",
      "rouge1:29.00\n",
      "rouge2:5.74\n",
      "rougeL:15.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print rouge score\n",
    "for root, dirs, files in os.walk('QMSum'):\n",
    "    for file in files:\n",
    "        if file == 'gpt3_truncate_squality_rouge.json':\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                rouge = f.read()\n",
    "                obj_rouge = eval(rouge)\n",
    "                parts = root.split(\"\\\\\")\n",
    "                print(parts[1]+'\\\\'+parts[2])\n",
    "                print(f\"rouge1:{obj_rouge['rouge1'].mid.fmeasure * 100:.2f}\")\n",
    "                print(f\"rouge2:{obj_rouge['rouge2'].mid.fmeasure * 100:.2f}\")\n",
    "                print(f\"rougeL:{obj_rouge['rougeL'].mid.fmeasure * 100:.2f}\")\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Print rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print rouge score\n",
    "for root, dirs, files in os.walk('QMSum'):\n",
    "    for file in files:\n",
    "        if file == 'pri_rouge.json':\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                rouge = json.load(f)\n",
    "            for key, value in rouge.items():\n",
    "                parts = root.split(\"\\\\\")\n",
    "                print(parts[1]+'\\\\'+parts[2])\n",
    "                print(key)\n",
    "                print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Print bert score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print bert score\n",
    "for root, dirs, files in os.walk(\"QMSum\"):\n",
    "    for file in files:\n",
    "        if file == \"bart_bert_score.json\":\n",
    "            with open(os.path.join(root, file), \"r\") as f:\n",
    "                bert = json.load(f)\n",
    "            average_f1 = bert['average_f1']\n",
    "            parts = root.split(\"\\\\\")\n",
    "            print(parts[1]+'\\\\'+parts[2])\n",
    "            print(f\"{average_f1 * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Print GPT eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QMSum\\dense\\MIN\\evaluation\\gpt3_rel_truncate_gpteval.json\n",
      "28.23\n",
      "QMSum\\LLM-embedding\\MIN\\evaluation\\gpt3_rel_truncate_gpteval.json\n",
      "30.85\n",
      "QMSum\\oracle\\evaluation\\gpt3_rel_truncate_gpteval.json\n",
      "32.92\n",
      "QMSum\\sparse\\MIN\\evaluation\\gpt3_rel_truncate_gpteval.json\n",
      "26.92\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "metric_list = ['coh', 'con', 'flu', 'rel']\n",
    "metric_index = 3\n",
    "model_list = ['bart_', 'gpt3_','llama_','pri_']\n",
    "model_index = 1\n",
    "suffix = model_list[model_index] + metric_list[metric_index] + '_truncate_gpteval.json'\n",
    "for root, dirs, files in os.walk(\"QMSum\"):\n",
    "    for file in files:\n",
    "        if file == suffix:\n",
    "            with open(os.path.join(root, file), \"r\") as f:\n",
    "                gpteval = json.load(f)\n",
    "            value_li = []\n",
    "            for key, value in gpteval.items():\n",
    "                value_li.extend(value)\n",
    "            # average = sum(value_li) / len(value_li)\n",
    "            average = gpteval['average'][0]\n",
    "            # gpteval['average'] = [average]\n",
    "            print(os.path.join(root, file))\n",
    "            print(f'{average * 10:.2f}')\n",
    "            # with open(os.path.join(root, file), \"w\") as f:\n",
    "            #     temp = json.dumps(gpteval)\n",
    "            #     f.write(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename\n",
    "for root, dirs, files in os.walk(\"../ODMDS_share_evaluate/SQuALITY\"):\n",
    "    if root.endswith('summary'):\n",
    "        prefix = 'newSummary_'  # 要替换的前缀\n",
    "        new_prefix = 'gpt3_summary_'  # 新的前缀\n",
    "        for filename in files:\n",
    "            if filename.startswith('newSummary_'):\n",
    "                new_filename = new_prefix + filename[len(prefix):]\n",
    "                os.rename(os.path.join(root, filename), os.path.join(root, new_filename))\n",
    "    if root.endswith('evaluation'):\n",
    "        new_prefix = 'gpt3_'  # 新的前缀\n",
    "        for filename in files:\n",
    "            if filename.startswith('evaluate'):\n",
    "                new_filename = new_prefix + filename\n",
    "                os.rename(os.path.join(root, filename), os.path.join(root, new_filename))\n",
    "    for filename in files:\n",
    "        if filename == 'all_results.json':\n",
    "            os.remove(os.path.join(root, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Delete file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename\n",
    "for root, dirs, files in os.walk(\"../ODMDS_share_evaluate\"):\n",
    "    for filename in files:\n",
    "        if 'evaluate_' in filename:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"QMSum\"):\n",
    "    if not files and not dirs:\n",
    "        with open(os.path.join(root, 'empty.txt'), 'w') as f:\n",
    "            f.write('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test random index of primera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('QMSum/randomIndex/index.json', 'r') as f:\n",
    "    random_index_list = json.load(f)\n",
    "for root,dirs,files in os.walk('QMSum'):\n",
    "    for filename in files:\n",
    "        filepath=os.path.join(root,filename)\n",
    "        if 'pri_rel' in filename:\n",
    "            with open(filepath,'r') as f:\n",
    "                rel=json.load(f)\n",
    "            rel=[rel['Summary'][index] for index in random_index_list]\n",
    "            print(np.mean(rel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
