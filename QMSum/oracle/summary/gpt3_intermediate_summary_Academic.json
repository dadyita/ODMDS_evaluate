[
    [
        "During the meeting, there was discussion about XML tools, data quality, and the current XML format for linking up different components in data. F and A expressed their opinions on the current XML format. \n\nIn regards to XML tools, it was mentioned that various XML tools have been installed in different languages, such as Java and Perl. These tools can be used to extract information from XML files, including user information, meeting information, and transcripts. It was also mentioned that the current XML format for transcripts is not ideal as it does not mark the start and end of each utterance, making it difficult to use in other tools. There was a suggestion to improve the format by adding start and end markers, as well as implementing an indirect timeline feature. \n\nIn terms of data quality, it was mentioned that additional transcribers have been hired and progress is being made. However, there is still more work to be done and it will take another week to complete the double checking process. It was also mentioned that the first five EDU meetings have been processed and chunked up, and one of them has already been sent to IBM. The others will be sent once the first one is approved.\n\nRegarding the CrossPad devices, it was mentioned that they have been used a couple of times in the past but there hasn't been much usage recently. Some participants expressed their opinions on the devices, with one mentioning that they found the pen to be noisy. There was also discussion about the need for infrastructure to synchronize the time on the CrossPad with the recording time. It was suggested that using voice synchronization or other methods could be an alternative, but it would still require additional infrastructure to mark the synchronized points in the transcript.\n\nOverall, the meeting provided insights into the current status of XML tools, data quality, and the use of CrossPad devices. Opinions were shared on the current XML format and potential improvements.",
        "The discussion in the meeting revolved around XML tools, data quality in meetings, and the current XML format for linking different components in data. The participants discussed the possibility of synchronizing meetings and infrastructure, as well as the potential benefits of using tools like SpeechCorder for note-taking. There was a suggestion to use a PDA with a wireless card for easy synchronization with a Linux server. The participants also discussed the current XML format and its limitations, such as the difficulty of getting people to routinely use it. \n\nOpinions on the current XML format were provided by F and A. F expressed interest in using a PDA with a wireless card for synchronization and noted that buttons would be convenient for making notes. A suggested using a PDA with a wireless card as well, but also mentioned the possibility of using buttons for convenience. Both F and A seemed open to the idea of using a different format, such as the one used by ATLAS, if it was more suitable for their needs and had tools that others could use as well. \n\nIn terms of data quality, the participants discussed the challenges of dealing with large files, especially for frame-level data. It was suggested to use a more compact format like P-files for frame-level features. The participants also mentioned the need to consider different types of annotations, such as prosody and phone-level alignments, and how to represent multiple speakers in the XML format. \n\nOverall, the participants were open to exploring different options for XML tools and formats, considering factors such as ease of use, compatibility with existing tools, and the ability to share the format with others.",
        "During the meeting, the discussion revolved around XML tools, data quality, and the current XML format for linking different components in data. The participants, F and A, shared their opinions on the current XML format.\n\nThe conversation started with a question about how to express hierarchical relationships between utterances and the words within them using the NIST format. The participants discussed the possibility of using another structure to indicate which words belong to each utterance. They also mentioned the use of annotations to mark segments or sentences within the utterances.\n\nThe participants then talked about the efficiency of extracting information from such a structure. They mentioned the need for tools that can efficiently search and extract information based on the structure. They discussed the possibility of using a structured XML query language to search for content and structural positions in XML documents.\n\nThe discussion then shifted to the challenges of merging different annotations of the same word sequence. The participants mentioned the difficulty of reconciling annotations from different versions of the data and the importance of having a tool that can merge these annotations. They also discussed the idea of using time marks to tie annotations to the word sequence.\n\nThe participants also touched upon the idea of using a relational database to store the XML data, which would allow for more advanced search capabilities. However, they also acknowledged that a flat file format with search tools might be sufficient for their current needs.\n\nIn terms of the current XML format, the participants expressed the need for a format that can be easily extended and integrated with other tools and databases. They mentioned the ATLAS external file representation as a potential format to consider.\n\nIn conclusion, the participants discussed the challenges of representing and linking different components in data using XML tools. They also shared their opinions on the current XML format and the need for efficient search and merging capabilities."
    ],
    [
        "During the meeting, there was discussion about electronics, specifically about the functionality and design of head-mounted microphones. There was also mention of the need for a labeling machine to improve the quality of labeling. The topic of intentionality was briefly mentioned in relation to the need for people with a clear accent to participate in the meeting. \n\nThe main focus of the meeting was on transcription. The group discussed using the \"Transcriber\" tool from a French group for converting audio to text. They talked about the process of transcribing, including marking the timing, speaker identification, and other transcription conventions. The idea of using speech recognition technology was also brought up, but it was noted that the recognition quality may not be sufficient. \n\nThere was discussion about the possibility of getting linguistics students and undergrads to do the transcription, but it was mentioned that it would require a post pass to ensure accuracy. The issue of funding for transcription was also mentioned, with the suggestion of going ahead with transcription assuming there will be funding in the future. The group also discussed the possibility of merging transcription with speech recognition technology. \n\nOverall, the meeting focused on the practical aspects of transcription and the need for further research and decision-making on how to proceed with transcription and distribution of the data.",
        "During the meeting, the discussion revolved around electronics, specifically the topic of transcription conventions and tools for generating transcriptions automatically. The participants discussed the feasibility and cost of using external transcription services, such as Cyber Transcriber, which was estimated to cost around $300 per hour for the type of technical transcription needed. The idea of using graduate students to transcribe the meetings was also discussed, with the estimated rate being around $10 per hour. It was mentioned that graduate students could potentially provide more accurate transcriptions since they have a better understanding of the content and context. The participants also discussed the limitations of existing transcription software, such as the inability to handle multiple speakers and the need for more advanced user interfaces. The topic of intentionality was briefly mentioned, with the participants discussing the need to indicate speaker changes and continuity of thought in the transcriptions. It was suggested that arrows could be used to show continuation of a thought across multiple utterances. The participants also mentioned the possibility of creating a website or using Doctor Speech to share the transcription conventions and tools developed during the meeting. Overall, the meeting focused on finding cost-effective and efficient ways to generate accurate transcriptions of the meetings.",
        "In the meeting, the discussion revolved around electronics, the segment, the results from the experiments, and the topic of intentionality. The participants discussed the use of electronics in their project, particularly the use of TCL and the importance of having access to data. They also discussed the need for a consolidated system to organize and access the various web and document resources related to the project. The participants also talked about the design of a cabinet to house the equipment and the possibility of using a laptop with wireless capabilities as a user interface. They also discussed the different features they were considering in their project, such as prosody, discourse, verb choice, landmark-in-ness, context, time of day, and openness to suggestions. They also touched on the idea of nice walls as a feature related to architecture. Overall, the participants were exploring different ideas and options for their project and considering the best approach to achieve their goals.",
        "In the meeting, the team discussed various aspects related to electronics. They discussed the concept of belief-nets and the challenges of setting up probabilities for them. They also talked about the idea of adding a middle layer to the belief-net in order to capture more complex concepts, such as the intentionality of a person's actions. They discussed the hidden variables that they came up with, such as whether someone is a tourist or running an errand, and how these variables would affect the overall mode of the belief-net. They also talked about the limitations of the current implementation and the need for further work, such as learning the probabilities and refining the structure of the belief-net. Overall, the discussion revolved around the segmentation of the problem, the results from the experiments, and the topic of intentionality.",
        "In the meeting, the discussion revolved around electronics, specifically the segment related to experiments and the topic of intentionality. The participants discussed the interaction between discourse processing and other aspects of electronics, such as SmartKom. They explained how the intention of the speaker is determined and how the discourse history module helps with anaphora resolution and filling in missing information. They also discussed the importance of understanding the plans and intentions of individuals in discourse processing. Additionally, they touched on the topic of trains and their routes, mentioning the Transcontinental Railroad and discussing the reputation of Amtrak. \n\nIn terms of the experiments, the participants shared the results of different training sets and language combinations. They discussed the impact of using PLP and delta features, as well as the performance of different languages in the experiments. They also mentioned the use of on-line normalization and its effect on accuracy. The participants conducted experiments combining different features and neural network outputs, comparing the results with the baseline performance. They found that adding MSG did not significantly improve accuracy in certain conditions. They also discussed the potential impact of training the neural network with PLP, delta, and delta-delta features. \n\nOverall, the participants explored various aspects of electronics, including discourse processing, intentionality, train routes, and the results of experiments. They discussed the importance of different features and training sets in improving accuracy and the potential challenges of training on different languages and mismatched data.",
        "During the meeting, the discussion focused on the topic of electronics, specifically regarding the segment, the results from the experiments, and the concept of intentionality. The researchers discussed the different noise conditions and how it affected the results of their experiments. They found that using a new neural net configuration improved the results slightly. They also discussed the use of delta features and how it could potentially bring the results to a similar level as the difficult cases. They further discussed the experiments with the silence output and found that it provided less information from the neural network but still improved the results. They also mentioned the use of delta and delta-delta configurations and found that the second configuration had a higher accuracy. The researchers then discussed the suggestion of scaling the variances associated with the streams in order to effectively scale the streams themselves. They also talked about the possibility of using different weights for the MLP and the need for further experiments to explore this. They also discussed the intentionality of the neural network and how changing the variances could affect the range of things. The researchers then moved on to discuss their plans for future work, including discussions with other researchers and the possibility of plugging their networks into the OGI system. They also talked about the use of multi-band and TRAPS systems, as well as training networks with larger contexts. They mentioned the need to work on the Finnish language and determine which MLP would be the best across different languages. They also discussed the need for further testing and the potential of using different training targets, such as articulatory or soft targets. The researchers also discussed the importance of addressing CPU and memory issues and the need to combine their work with the OGI system. They concluded the meeting by discussing the potential future work and the challenges they may face. Overall, the meeting covered a wide range of topics related to electronics, including experiments, results, and future plans.",
        "During the meeting, the discussion revolved around various topics related to electronics, the segment, the results from experiments, and the concept of intentionality. The participants discussed the status of the CPU and disk, as well as some problems they encountered, which were correctable by restarting the script. They also talked about the data collection for the project and the involvement of different students and subjects. The focus then shifted to the synthesis of the system, with some issues being resolved and a meeting planned for the next day. The participants also discussed the PRM (Probabilistic Relational Model) and the structure and dependencies of its classes. They talked about the output and decisions made by the PRM and the potential for integration with other systems. The discussion then moved to the topic of content management systems and the challenges of shared interpretation of keywords and taxonomies. The participants talked about the need for improvement in this area and the potential for a smart FAQ system that can provide context-specific answers. They discussed the importance of reusing existing domain knowledge and ontologies and the potential for probabilistic approaches in document retrieval. The participants also talked about the idea of querying multiple search engines and the possibility of a peer-to-peer approach for information sharing. The meeting concluded with the participants discussing potential collaborations and further exploration of related topics such as FrameNet and information extraction."
    ],
    [
        "The main thesis of the discussion was focused on analyzing overlapping speech in conversations. The professor expressed his interest in studying the effects of overlapping speech on energy levels and the potential for using pitch and harmonicity as indicators. The professor also mentioned the importance of normalizing the data and considering the duration of the overlapping segments. The professor and the participants discussed the possibility of categorizing different types of overlaps based on speaker style and the need for time marks in the analysis. They also mentioned the potential for hiring an undergraduate student to help with the labeling process. Additionally, the participants discussed the possibility of using forced alignment software to aid in the analysis. The conversation ended with a discussion about the new forms that were proposed for data collection, with a focus on the order of the date and time fields.",
        "The main thesis of the discussion was about making revisions and improvements to a thesis proposal. The professor's opinions on the 12 second mean, possible hypothesis, concluding comments, and future work were not mentioned in the meeting.",
        "The main thesis of the discussion was the use of construal and metonymy in language understanding, specifically in the context of a tourist domain. The professor mentioned the importance of combining constructions with a general construal mechanism to achieve orthogonality and scalability in language understanding.\n\nThe professor's opinions on the 12 second mean were positive, as it showed promising results when used in the SmartKom system. The test conducted by the student involved using past and present frames to calculate the mean, and compared it to a twelve-second centered window. Although there was a slight drop in performance, it was considered encouraging for the use of the 12 second mean in an interactive system.\n\nRegarding possible hypotheses, the discussion touched on the idea of using the \"where\" question in the tourist domain as an example of construal, differentiating between locational and instructional requests. The professor also mentioned the possibility of finding a nice metaphorical use of \"where\" in the tourist's domain.\n\nIn concluding comments, the professor mentioned the importance of construal in the tourist domain, even though it may involve less metaphor and more metonymy. The professor also discussed the possibility of looking at mental space examples and temporal references in the tourist domain.\n\nAs for future work, the professor mentioned the visit of Hans-Guenter and the potential for longer collaboration. The professor also suggested looking at the mental space examples in the second data run and exploring the use of spectral subtraction in the SmartKom system.\n\nOverall, the discussion focused on the use of construal and metonymy in language understanding, with specific applications in the tourist domain and the SmartKom system. The professor's opinions were positive, and there were suggestions for future research and exploration.",
        "The main thesis of the discussion was focused on noise compensation techniques, specifically Wiener filtering and spectral subtraction. The professor and the graduate students were discussing their experiments and findings related to these techniques. \n\nThe professor expressed his opinion that if the training data and test data have different lengths, it may not be beneficial to train the models using a longer time. He suggested using a filtering perspective and designing a filter that can adapt to different lengths of speech. He also mentioned the possibility of using multiple passes in the system to improve performance for different utterances. \n\nThe graduate student working on Wiener filtering shared his findings that applying a single stage Wiener filter did not show significant improvement over the baseline. However, adding a second stage of Wiener filtering with a smoothed filter resulted in a 56.46% improvement on the SpeechDat-Car dataset. He also mentioned the importance of using a proper voice activity detector (VAD) for accurate noise estimation. \n\nAnother graduate student discussed his work on spectral subtraction. He explained the technique of subtracting an estimated noise spectrum from the signal spectrum based on the SNR. He mentioned the challenges of dealing with negative values after subtraction and the need for thresholding and adding noise back to avoid negative values. However, he noted that spectral subtraction alone did not yield significant improvement and further investigation was needed.\n\nOverall, the discussion revolved around different noise compensation techniques, their effectiveness, and potential improvements. The professor and graduate students shared their opinions and findings on the topic.",
        "The main thesis of the discussion was focused on improving the performance of the speech recognition system through various techniques and approaches. The professor and the PhD students discussed different methods such as spectral subtraction, noise estimation, voice activity detection, and feature combination.\n\nRegarding the 12-second mean, the professor mentioned that it would be the opposite of leaving negative numbers in the subtraction for a bin. The PhD students agreed with this statement. They also discussed the possibility of re-computing negative values using interpolation or reflecting them back up for a full wave rectification.\n\nIn terms of possible hypotheses, the PhD student mentioned trying different smoothing techniques for spectral subtraction and optimizing the time constant for smoothing. They also discussed the idea of training a neural network for spectral subtraction and using different noise estimation techniques.\n\nIn their concluding comments, the professor mentioned the potential benefits of using neural networks for noise estimation and the importance of considering latency in the system. The PhD student mentioned the need for further experimentation and optimization of parameters to improve the system's performance.\n\nAs for future work, they discussed the possibility of developing a better voice activity detector and training a neural network on all the available data. They also mentioned the idea of using a second silence detection system, but the feasibility was limited due to the delay in the system.\n\nOverall, the discussion revolved around exploring different techniques and approaches to improve the speech recognition system, such as spectral subtraction, noise estimation, and voice activity detection. The professor and the PhD students discussed their findings, hypotheses, and potential future work to enhance the system's performance.",
        "Based on the given meeting, the main thesis of the discussion is the idea of creating a system that can generate questions based on its understanding of certain conditions and intentions. The professor and the team discussed the possibility of the system asking itself questions, entering a dialogue with itself, and learning from the process. They also mentioned the need to fix the system and write a paper for a conference on neurolinguistics and cognitive architecture. The professor expressed interest in exploring different architectures and potential improvements for future work.",
        "The main thesis of the discussion was to determine the structure and content of a paper that would discuss the topic of \"where is X\" questions and the complexities and ambiguities that arise from them. The professor's opinion was that these questions are not as straightforward as they may seem and that previous research on the topic has been inadequate. The professor proposed designing a belief net that would analyze the discourse and ontology of these questions to determine the speaker's intent and provide a helpful response. The professor also mentioned the possibility of continuing the research and data collection after the current research assistant leaves in August. Overall, the discussion revolved around the structure of the paper, the analysis of \"where is X\" questions, and the future direction of the research."
    ],
    [
        "During the meeting, various topics were discussed regarding the transcripts. The discussion started with technical issues related to microphone channels and battery power. Then, the participants talked about the need to turn on microphones and the availability of channel numbers. The conversation then moved on to discussing the gain levels and potential adjustments to improve audio quality. \n\nThe participants mentioned the absence of one member, Jane, and debated whether to wait for her or start the meeting without her. They also discussed the upcoming DARPA meeting and the need to prepare for it, particularly in terms of the transcription interface and mock-up question answering. \n\nThe conversation shifted to the hiring of transcribers and the progress of transcribing the meetings. They mentioned the attrition of transcribers and the need to hire more to maintain productivity. They also discussed the organization of meeting information and the creation of a spreadsheet to track the status of meetings and transcriptions. \n\nThe participants then discussed the progress of Don's project, which involved analyzing transcripts and exploring features such as prosody and word frequency. They mentioned the challenges of working with limited data and the need to gather more for better results. They also discussed the potential of using time position as a feature and the differences in pitch features between the Meeting Recorder and Switchboard data. \n\nThe conversation briefly touched on the possibility of changing insertion penalty settings but concluded that it did not significantly impact the results. The participants mentioned the need to tune the Gaussian system for better training. They also discussed the use of vocal tract length normalization and its positive effect on accuracy. \n\nOverall, the meeting covered a range of topics related to transcripts, including technical issues, hiring transcribers, progress updates, and future plans for improvement.",
        "During the meeting, there was a discussion about the improvement of the transcripts. The participants discussed the need to compare the current transcripts with a baseline and the possibility of using VTL in the training process. They also talked about the different effects of various features and the potential improvements that could be achieved through optimization. The meeting also touched on the idea of using the transcripts for research purposes, particularly in relation to understanding language inference structures. The participants mentioned the challenges of anonymizing the data and the need to be cautious when mentioning names in the transcripts. Overall, the meeting focused on the discussion of research ideas and the potential applications of the transcripts.",
        "During the meeting, there was a discussion about the transcription of the audio data. The focus was on identifying and marking speaker overlaps in the transcripts. It was mentioned that there were approximately 300 speaker overlaps in a 45-minute session. However, the speaker overlaps were not counted in the 12-minute transcription that was done. The meeting participants discussed the possibility of using an energy detector with a median filter to automatically detect speaker overlaps. It was suggested that this could speed up the transcription process. The participants also discussed the idea of using echo cancellation to improve the quality of the audio recordings. It was suggested that someone should try implementing echo cancellation to see if it could reduce the effect of echoes and improve the clarity of the recordings. Additionally, the participants mentioned that there were approximately 10 hours of recorded data, with plans to record more meetings in the future. The idea of adapting the system for recognizing digits was also mentioned, and it was suggested that someone should work on adapting the system using the recorded digit data. Overall, the meeting focused on finding ways to improve the transcription process and enhance the quality of the audio recordings.",
        "In the meeting, they discussed several topics including the status of the transcripts, the requirements for editing bleeps in the recordings, and the possibility of getting more data by recording additional meetings. They mentioned that they currently have about 35 hours of recorded data, including the digits. They also mentioned that they are planning to replace the wired microphones with wireless ones and possibly get another wireless setup. They discussed the progress of the transcriptions and the need for clean and consistent data. They also mentioned implementing new conventions for handling numbers, acronyms, and different types of comments in the transcripts.",
        "In the meeting, the discussion revolved around the transcripts and the requirements and agreements related to them. Postdoc F mentioned that they had a convention in place for handling the transcripts, but they hadn't been systematically asking people to do it. They believed that handling the transcripts through a filter would be more efficient and less error-prone than having a transcriber type them in. They discussed how to handle numbers and acronyms and agreed that the letters should be separated by spaces and potentially followed by a curly bracket to indicate what they represent. They also discussed the possibility of providing a gloss for clarity. The meeting also touched on the importance of spot checks to ensure the accuracy of the transcripts. \n\nRegarding the distribution of the transcripts, there was a discussion about the best way to provide access to participants. It was suggested that each participant would have a single password and would only be able to access the transcripts for the meetings they attended. There was a debate about whether to provide printouts or rely solely on the web interface. The concern was that making it too easy for participants to edit the transcripts could lead to unnecessary bleeping of sections. It was suggested that participants could indicate their approval or objection via email, and only those who wanted to review the transcripts would be given access to the web interface. \n\nThere was also a discussion about the legal aspects of the transcripts, including whether a concrete signature was needed or if email approval was sufficient. It was determined that email approval would be acceptable. The meeting concluded with a discussion about the need for accuracy in the transcripts and the possibility of participants listening to the recordings to ensure accuracy. It was noted that the technical nature of the discussions made it unlikely that there would be many objections or requests for editing."
    ],
    [
        "In the given meeting, the opinions of PhD F and PhD B on the topic of generating queries automatically were not explicitly discussed. However, there were discussions on related topics such as the discomfort of headphones, note-taking during meetings, and the potential use of summaries and visual information in data collection efforts.\n\nPhD B mentioned that headphones can be uncomfortable and cause headaches, while Grad G mentioned that he hasn't figured out a solution to this problem. PhD F made a comment about \"temple squeezers,\" which was agreed upon by PhD B and Grad G.\n\nThe meeting then moved on to discussions about recording and transcribing meetings, with PhD E suggesting the use of CrossPads to capture different types of information. PhD B mentioned the poor quality of handwriting recognition, which could be a challenge in generating queries from written notes. Professor A suggested using summaries of meetings to generate queries, and PhD B agreed that summaries could drive the queries.\n\nThere were also discussions about the challenges of generating queries without an interactive system and the motivation factor of participants in providing queries. PhD E suggested that retrospective queries about previously overlooked important points could be generated from the complete data capture. Grad G mentioned the difficulty of generating queries without a system and the potential need for an expert to listen to the meeting and identify important points.\n\nThe meeting then moved on to discussions about the role of note-takers and the possibility of using digital cameras to capture visual information during meetings. PhD B suggested having a dedicated note-taker or someone attending the meeting to take notes for research purposes. Postdoc H suggested having multiple people summarize the meeting orally immediately after it ends, while PhD B emphasized the importance of having records of what was written on the board during the meeting.\n\nThere were also discussions about the challenges of implementing a system for generating queries, the need for a database of meeting recordings, and the potential use of visual information in the database. PhD B mentioned DARPA's interest in fusing gesture and face recognition for this type of task, and Professor D agreed that visual information could be useful in the future.\n\nIn summary, while the opinions of PhD F and PhD B specifically on the topic of generating queries automatically were not explicitly discussed in the meeting, there were discussions related to note-taking, summaries, and the potential use of visual information in data collection efforts. The meeting participants recognized the challenges and potential benefits of generating queries and suggested various approaches, such as using summaries and complete data capture, to address these challenges.",
        "During the meeting, PhD F and PhD B had different opinions on the topic of generating queries automatically. PhD B mentioned that they had talked to a student who had recorded a meeting using cameras and believed that it would be valuable to collect this data for future processing. However, PhD B also acknowledged that not everyone would want to be filmed, and suggested alternatives such as covering faces or simply recording audio. PhD F questioned whether it would be feasible to generate queries automatically from meeting summaries, as this would require further research. PhD B suggested that Landay and his group, who specialize in user interface, should be responsible for figuring out how to generate queries. The group also discussed the idea of asking participants to provide a summary of the most interesting thing they learned from the meeting, as this could provide additional insights. Overall, it was agreed that more thought and discussion was needed to determine the best approach for generating queries automatically.",
        "In the meeting, PhD F and PhD B expressed their opinions on the topic of generating queries automatically. PhD F did not provide a specific opinion on the matter. However, PhD B discussed the level of the query, stating that it could range from low-level to high-level. They also mentioned the need to have a way of identifying the type of query and whether it requires a one-word answer or is more complex. PhD B also highlighted the importance of asking queries that are meaningful for people and suggested starting with keywords and gradually working up to more detailed queries. \n\nProfessor D expressed a different viewpoint, suggesting that the goal should be to understand what people would like to know about the data and to focus on research projects that explore areas that are not yet known. They mentioned the possibility of using a Wizard of Oz approach to gather information on what people would like to know. \n\nIn summary, PhD F did not provide a specific opinion, while PhD B emphasized the importance of meaningful queries and starting with keywords. Professor D suggested focusing on understanding people's interests and exploring unknown areas."
    ],
    [
        "During the meeting, the discussion on latency in the system was not mentioned. Therefore, there is no information on why the professor had an issue with latency or the duration of the latency.",
        "During the meeting, the discussion on latency in the system was brought up. The professor had an issue with latency because it adds delay to the system. The duration of the latency was mentioned to be around 50 milliseconds. The meeting also discussed the use of spectral subtraction and Wiener filtering for noise reduction. The professor mentioned that there can be negative values in the spectral subtraction process, which indicates that more noise was subtracted than there actually was. The professor also questioned the impact of the musical noise caused by the spectral subtraction. The meeting touched on the idea of using a neural network for noise reduction and how it could potentially improve the system. The discussion then shifted to the use of noise estimation techniques without the need for voice activity detection. One technique mentioned was using minima of energy in frequency bands to estimate the noise level. The assistant also mentioned the implementation of spectral whitening to reduce musical noise. The meeting concluded with the mention of developing a better voice activity detector and the possibility of training a neural network on all the available data. The use of different features for the voice activity detector, such as spectral slope and correlation between bands, was also discussed."
    ],
    [
        "In the meeting between Grad D and Grad C, the key points discussed about the roles of the computer and wizard were related to transcription status and the process of transcribing meetings. Grad E's contribution to the discussion on storage disks was about the availability of space on his computer and the potential use of external drives. Grad B provided updates on the meeting regarding disk storage, including the need for more space and the possibility of adding more disks to the machine room.\n\nAdditionally, Grad D mentioned the results of testing the PLP configuration on development test data and the recognition performance being the same as the Mel Cepstra front-end. Grad D also discussed the combination of systems and the improvement achieved through the use of N-best ROVER. The discussion then shifted to the performance of the Hub-five system on digits, which was attributed to the inclusion of read speech data in the training set. Grad D also mentioned the possibility of adding more read speech data to improve the system's performance.\n\nLastly, Grad D and Grad F discussed the progress on the tandem system and the conversion of alignments into label files for training a new net. They also mentioned the potential benefits of reducing the phone set dimensions and trying deltas on the tandem features.\n\nOverall, the meeting covered various aspects of transcription, storage disks, system performance, and future plans for system development.",
        "In the meeting between Grad D and Grad C, the key points discussed about the roles of the computer and wizard were that they were looking to improve the system compared to previous versions. They discussed the importance of having a well-tuned dictionary and the impact it can have on reducing error rates. They also mentioned the importance of having accurate alignments and how they initially obtained them from Decipher. Grad E's contribution to the discussion on storage disks was not mentioned in the given meeting. Lastly, Grad B provided updates on the meeting regarding disk storage, but the details of these updates were not mentioned in the given meeting.",
        "In the meeting between Grad D and Grad C, the key points discussed about the roles of the computer and wizard were not explicitly mentioned. However, they did discuss the need for more disk space to record meetings and the process of archiving and deleting files to free up space. Grad E's contribution to the discussion on storage disks was not mentioned in the meeting. Lastly, Grad B provided updates on the progress of archiving and mentioned the need for new disk racks and disks to increase storage capacity.\n\nOverall, the meeting focused on the challenges of managing disk resources and finding solutions to ensure enough space for recording meetings. The discussion on storage disks and archiving highlighted the need for efficient data management.",
        "During the meeting between Grad D and Grad C, the key points discussed about the roles of the computer and wizard were not clearly mentioned. However, there was a discussion about the issue of clean signals coming from one direction and the possibility of using a turntable to study binaural problems. Grad E contributed to the discussion on storage disks by mentioning the axis of access and the complexity of the topology. Grad B provided updates on the meeting on disk storage, but these updates were not clearly stated in the given meeting.",
        "In the meeting between Grad D and Grad C, the key points discussed about the roles of the computer and wizard were that they both played different roles in the wizard test data test. The same person acted as the wizard, pretending to be a system in one instance and pretending to be a human in another. The system was designed to break down at a certain point, and the tasks for the subjects varied depending on whether they were interacting with the computer or a human. It was noted that the reading task was too long and needed to be shortened. It was also suggested to have a better introduction for the wizard when it is called. Another suggestion was to switch tasks between subjects to gather more diverse data.\n\nGrad E contributed to the discussion on storage disks by mentioning that there may be a problem with a reading task when using the disk storage system. The group was already aware of this potential issue and was planning to address it. \n\nIn terms of updates provided by Grad B regarding the meeting on disk storage, it is mentioned that Grad B had signed up for a Wednesday slot to discuss the system design and involve linguists in the discussion. It was suggested that the linguists could contribute to the understanding of the dialogue and the tasks involved. The meeting would also involve discussing the process of hiring Fey and organizing subjects for the tests.\n\nOverall, the meeting covered various aspects of the wizard test data test, including the roles of the computer and wizard, potential issues with the storage disks, and updates on the meeting related to disk storage. The group also discussed the need for better introductions, shorter reading tasks, and the potential for a cognitive dispatcher module to assist in decision-making."
    ],
    [
        "SmartKom is an architecture that consists of various modules, including a recognizer, parser, generator, and action planner. The meeting discussed the importance of integrating these modules and the challenges involved. The architecture aims to provide a seamless dialogue experience for users, allowing them to interact with the system naturally and receive accurate and relevant information.\n\nOne of the main focuses of the meeting was on the integration of the generator module, which is responsible for generating responses based on user input. The team discussed the different approaches to generating responses in English and German, and the challenges of adapting the grammars for English generation. They also discussed the importance of considering the user's intentions and preferences in generating responses.\n\nThe team also discussed the importance of the action planner module, which is responsible for planning and executing actions based on user requests. They talked about the need for a well-defined interface between the action planner and other modules, as well as the potential for external services to provide additional functionality.\n\nThe meeting also touched on the need for syntactic analysis in the language input pipeline. The team discussed different parsers and chunk parsers that could be used for this purpose, with a focus on finding a fast and robust solution.\n\nOverall, the meeting highlighted the importance of integrating the different modules of SmartKom and ensuring that they work together seamlessly. The team discussed the challenges and potential solutions for various aspects of the architecture, including language generation, action planning, and syntactic analysis.",
        "SmartKom is an architecture that consists of various modules and is designed to provide a deep semantic understanding of user input. The architecture includes a knowledge base from the Verbmobil system and uses modules like the template parser. The importance of SmartKom lies in its ability to understand user input and make decisions based on that understanding. The architecture includes a middle layer that helps in extracting hidden variables like whether the user is a tourist or a business person, whether they are in a hurry, etc. The middle layer also includes features like the discussion of admission fees and the presence of nice walls. The architecture uses belief nets to combine the information from the discourse, ontology, and situation to make decisions. The architecture also includes a context node that determines whether the user is a tourist or not. The system uses probabilities to calculate the likelihood of different modes of interaction, like entering, vista-ing, or tango-ing. The system is still in development and requires further fine-tuning of probabilities and the addition of more hidden nodes.",
        "SmartKom is an architecture that includes modules for various tasks, such as understanding user intentions, generating appropriate responses, and managing discourse history. The architecture includes a discourse model that keeps track of previous dialogue and helps with anaphora resolution and gesture coordination. The importance of SmartKom lies in its ability to handle complex dialogue scenarios and adapt to different user needs.\n\nDuring the meeting, the team discussed the need to differentiate between objects that have a door and those that are not public. They also talked about the importance of considering context and previous discourse when determining user intentions. They discussed the possibility of adding a \"has door\" property to objects and how it could affect other variables in the system.\n\nThe team also discussed the architecture and modules of SmartKom, including the discourse model and its role in understanding user intentions and coordinating gestures. They mentioned the need for a trial run with a wizard to test the system and refine the variables and features used in the architecture.\n\nOverall, SmartKom is an architecture that aims to handle complex dialogue scenarios and adapt to user needs. It includes modules for understanding user intentions, generating responses, and managing discourse history. The team is working on refining the variables and features used in the system and planning a trial run to test its effectiveness."
    ],
    [
        "During the meeting, the options discussed regarding the location of the recording equipment were not explicitly mentioned. The meeting primarily focused on the tasks and goals of the project, which involved recording subjects in a simulated scenario where they interact with a computer system to perform various tasks related to tourism in Heidelberg. The subjects would be provided with a map or a two-dimensional display showing their location and the location of various landmarks. They would then be given tasks such as finding a specific place, taking a picture, or getting information about an object. The subjects would interact with the computer system, which would provide responses based on their input. The intention of the subjects in performing these tasks was not explicitly discussed in the meeting. However, it was mentioned that the belief-net approach could be used to infer the intention of the subjects based on the features extracted from their language and discourse. The meeting also mentioned the possibility of collecting data from actual subjects in the future to further understand the factors influencing their intentions.",
        "During the meeting, several options and decisions were discussed regarding the location of the recording equipment. It was mentioned that one possibility is to have a room in the linguistics department or another building on campus where meetings can be recorded. This would allow for regular recordings of different types of meetings. It was also suggested to reach out to professors and graduate students on campus to see if they would be interested in having their meetings recorded. Another option mentioned was to use portable equipment and record meetings at different locations. The goal was to have a diverse set of meetings to work with for research purposes. It was noted that for certain types of analysis, such as summarization, a smaller set of meetings with consistent participants may be more suitable. The idea of having both close and distant microphones was also discussed, with the understanding that different research needs may require different recording setups. Overall, the decision was to aim for a combination of regular meetings in a dedicated location and recordings of other meetings at various locations to create a diverse and comprehensive corpus of data.",
        "In the meeting, the options discussed regarding the location of the recording equipment were to either bring people to the current location or to have the equipment brought to different locations. The decision made was to prioritize bringing people to the current location as it was already set up and underused. The idea of offering free lunch to incentivize people to come was also discussed and agreed upon. It was also mentioned that volunteers from Haas Business School could be contacted for assistance. \n\nIn terms of the disk resources, it was mentioned that there is currently enough space to record four more meetings on the current disk. The archiving of the Broadcast News files was also mentioned, which would free up space for five or six more meetings. It was also noted that a new disk rack with additional disks would be acquired to increase storage capacity. \n\nRegarding the project of recording meetings, the discussion focused on the need to detect and classify overlapping speech. The idea of using supervised clustering and building models for classification was proposed. It was suggested to start with a few selected features and visualize their separation in different classes before using more complex classifiers. It was also mentioned that temporal properties and transitions between different speech states could be informative. The use of a neural network or a decision tree as classifiers was discussed. The importance of analyzing and understanding the features and classifiers used was emphasized. It was suggested to start with equalized mixed speech for easier analysis and then assess the performance with PDA files.",
        "During the meeting, the options discussed regarding the location of the recording equipment were not clearly defined. However, there were some points mentioned that could be considered as options. One option mentioned was to try using both the close mixed and PDA speech files to see if some features work well with both. Another option discussed was the possibility of using a PDA with two microphones to detect speaker overlaps. It was mentioned that having two microphones could provide location information, but there were concerns about the accuracy of this method. Another suggestion was to use a turntable to place the PDA on, which would allow for different angles and positions. It was also mentioned that using a microphone array could provide more precise and reliable results, but this would require a more complex setup. \n\nIn terms of the decision made, it was not clearly stated in the meeting. However, it was mentioned that more research and exploration is needed to determine the best approach for the location of the recording equipment. It was also suggested that the decision could be influenced by the specific goals of the project and the preferences of the researchers involved. Additionally, it was mentioned that the anonymization of the data was an important consideration, especially for future publication and distribution of the recordings. It was suggested that a consistent approach should be taken to anonymize the data, but further discussion and decision-making is needed on this topic."
    ],
    [
        "In the meeting, there were discussions about several topics including the progress on the digit transcription project, the update on transcripts, filtering for the ASI refs, and the acoustic-phonetic analyses. \n\nRegarding the digit transcription project, the speaker mentioned that they are almost done with the TI-digits and will be finished in a couple of weeks. They also mentioned that there were some skipped digits that needed to be filled in. Once the project is done, they mentioned the need to train a recognizer and start working with the data.\n\nThere was also a discussion about the update on transcripts, which involved summarizing the discussions from a previous meeting and filtering the transcripts for certain references. The purpose of this update was to provide a more complete transcript that includes all the necessary information.\n\nIn addition, there was a brief discussion about the acoustic-phonetic analyses. The speaker mentioned the possibility of incorporating articulatory features into the analysis, as well as the challenges of capturing prosodic and cross-word modeling in the data. They also discussed the idea of having transcribers annotate these features and the potential benefits of doing so.\n\nOverall, the discussions in the meeting covered the progress and future plans for the digit transcription project, the update on transcripts, and the potential for incorporating acoustic-phonetic analyses into the research.",
        "The discussions in the meeting touched on various topics including the talk, neurons, transcriber pool, and acoustic-phonetic analyses. The meeting began with a discussion about problems with the channel and normalization in the talk. The speaker mentioned that there were issues with the system not working properly when the speaker doesn't talk much or doesn't talk at all. The speaker also mentioned that the evaluation of the system is difficult without any references and that they had done some training on a specific channel for evaluation purposes.\n\nThe meeting then shifted to discussing the possibility of transcribers doing meetings in terms of speech and nonspeech in specific channels. It was mentioned that the postdoc had already transcribed thirty minutes of one meeting with reference to individual channels. The speaker expressed the need for more meetings to ensure the system's accuracy and suggested having some minutes from different meetings. The postdoc suggested including minutes from twelve meetings, which would take about an hour. The transcribers would then adjust the time bands to make them more precise.\n\nThe meeting then moved on to discussing the transcription process and the need for a clean transcript format that can be used by the SRI recognizer. It was mentioned that there may be variations in the length of the units in the multi-channel format and that the units should not be overly long. The speaker proposed running the transcript through channelize to get it into the multi-channel format and then see if the units are sufficient for the recognizer. If not, further breakdown of the units may be required.\n\nThe meeting also touched on the use of microphones and the possibility of using new microphones. It was mentioned that two new microphones were ordered and would be tried out. The meeting concluded with a discussion about the upcoming thesis proposal and the need for revisions and additional content. The speaker mentioned the importance of receiving comments before the 29th and mentioned presenting a talk at EML about the work done in the meeting.\n\nOverall, the discussions in the meeting covered a range of topics related to the talk, neurons, transcriber pool, and acoustic-phonetic analyses.",
        "During the meeting, there were discussions about various topics including talk, neurons, transcriber pool, and acoustic-phonetic analyses. The speaker mentioned the NTL story, which seems to involve different ideas about how language understanding works. They also talked about the embodied and simulation approach, as well as monkeys in Italy. The speaker mentioned that they had a paper and slides to summarize certain information. They discussed stealing an X-schema from one of the participant's talks and mentioned the construction aspect and Bayes-net. There was also a mention of FMRI and looking at people's brains in a more direct way. The discussion then shifted to the thesis proposal, where the speaker mentioned the need for examples related to the tourist domain. They talked about the use of \"in\" and \"on\" in different contexts, such as \"in the bus\" and \"on the bus.\" They also discussed the use of \"out of\" and the different construals of the word \"bakery.\" The speaker expressed the need for examples that are more straightforward and less metaphorical. They mentioned the use of \"where\" in different contexts, such as asking about the location of a castle or the existence of a bakery. They also mentioned the use of \"where\" in a metaphorical sense, such as \"where can I find out about something.\" The speaker highlighted the importance of construal in the tourist domain and mentioned the scarcity of metonymy and polysemy examples in their database. They discussed the distinction between locational and instructional requests and expressed the need for lexical and mental space examples. The meeting ended with the plan to provide feedback electronically and wish the participant success on their trip."
    ]
]